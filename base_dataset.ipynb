{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8efb8459-33ee-40ec-9061-b7ba2f2adfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b4cfbf5-6df2-4424-95c5-1f90dada961d",
   "metadata": {},
   "outputs": [],
   "source": [
    "datanames = ['toys-split','sports-split','beauty-split']\n",
    "dataname = datanames[1]\n",
    "data_root = f\"/data2/wangzhongren/taolin_project/dataset/{dataname}\"\n",
    "output_root = f\"/data2/wangzhongren/taolin_project/data/{dataname}\"\n",
    "sample_filename = f\"{dataname}.item\"\n",
    "dataset_types = ['train','valid','test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92cf0a49-5794-478d-a368-396fd4600d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>sales_type</th>\n",
       "      <th>brand</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000031852</td>\n",
       "      <td>Toys &amp; Games</td>\n",
       "      <td>Coxlures</td>\n",
       "      <td>'Other Sports', 'Dance', 'Sports &amp; Outdoors'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000032050</td>\n",
       "      <td>missing</td>\n",
       "      <td>BubuBibi</td>\n",
       "      <td>'Sports &amp; Outdoors', 'Skirts', 'Clothing', 'Gi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0615302939</td>\n",
       "      <td>Sports &amp; Outdoors</td>\n",
       "      <td>NNG</td>\n",
       "      <td>'Sports &amp; Outdoors'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      item_id         sales_type     brand  \\\n",
       "0  0000031852       Toys & Games  Coxlures   \n",
       "1  0000032050            missing  BubuBibi   \n",
       "2  0615302939  Sports & Outdoors       NNG   \n",
       "\n",
       "                                          categories  \n",
       "0       'Other Sports', 'Dance', 'Sports & Outdoors'  \n",
       "1  'Sports & Outdoors', 'Skirts', 'Clothing', 'Gi...  \n",
       "2                                'Sports & Outdoors'  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对item数据集的特征列进行处理，包括填充缺失值，对类别的分割\n",
    "feat_keys = ['item_id', 'sales_type', 'brand','categories']\n",
    "sample_path = os.path.join(data_root,sample_filename)\n",
    "df_feat = pd.read_csv(sample_path, sep='\\t', header=0)\n",
    "df_feat.columns = [col.split(\":\")[0] for col in df_feat.keys()]\n",
    "df_feat= df_feat[feat_keys]\n",
    "# 不对类目特征做额外处理\n",
    "# for i in range(3):\n",
    "#     df_feat[f'category_{i+1}'] = df_feat['categories'].str.split(',').str.get(i).str.strip().fillna('Unknown')\n",
    "# df_feat = df_feat.drop(columns=['categories'])\n",
    "df_feat['sales_type'] = df_feat['sales_type'].fillna(\"missing\")\n",
    "df_feat[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "315bd360-ce4b-4acc-b7b8-cccd6dad0855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>label</th>\n",
       "      <th>dataset_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A3PMSRCL80KSA1</td>\n",
       "      <td>0000031852</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1KJ4CVG87QW09</td>\n",
       "      <td>0000031852</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AA9ITO6ZLZW6</td>\n",
       "      <td>0000031852</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          user_id     item_id  label dataset_type\n",
       "0  A3PMSRCL80KSA1  0000031852      1        train\n",
       "1  A1KJ4CVG87QW09  0000031852      1        train\n",
       "2    AA9ITO6ZLZW6  0000031852      1        train"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对训练用数据集进行初步处理，label,列名等等，检查是否有空值\n",
    "df_all = pd.DataFrame()\n",
    "for dataset_type in dataset_types:\n",
    "    data_filename = f\"{dataname}.{dataset_type}.inter\"\n",
    "    data_path = f\"{data_root}/{data_filename}\"\n",
    "    df_data = pd.read_csv(data_path, sep='\\t', header=0)\n",
    "    df_data.columns = [col.split(\":\")[0] for col in df_data.keys()]\n",
    "    df_data['label'] = (df_data['rating']>3).astype(int)\n",
    "    df_data = df_data.drop(columns = ['rating'])\n",
    "    df_data = df_data.drop(columns = ['timestamp'])\n",
    "    df_data['dataset_type'] = dataset_type\n",
    "    df_all = pd.concat([df_all, df_data], ignore_index=True)\n",
    "df_all[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3aa8ac1b-9f59-479d-9802-21c60321c621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>label</th>\n",
       "      <th>dataset_type</th>\n",
       "      <th>sales_type</th>\n",
       "      <th>brand</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A3PMSRCL80KSA1</td>\n",
       "      <td>0000031852</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>Toys &amp; Games</td>\n",
       "      <td>Coxlures</td>\n",
       "      <td>'Other Sports', 'Dance', 'Sports &amp; Outdoors'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1KJ4CVG87QW09</td>\n",
       "      <td>0000031852</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>Toys &amp; Games</td>\n",
       "      <td>Coxlures</td>\n",
       "      <td>'Other Sports', 'Dance', 'Sports &amp; Outdoors'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AA9ITO6ZLZW6</td>\n",
       "      <td>0000031852</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>Toys &amp; Games</td>\n",
       "      <td>Coxlures</td>\n",
       "      <td>'Other Sports', 'Dance', 'Sports &amp; Outdoors'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          user_id     item_id  label dataset_type    sales_type     brand  \\\n",
       "0  A3PMSRCL80KSA1  0000031852      1        train  Toys & Games  Coxlures   \n",
       "1  A1KJ4CVG87QW09  0000031852      1        train  Toys & Games  Coxlures   \n",
       "2    AA9ITO6ZLZW6  0000031852      1        train  Toys & Games  Coxlures   \n",
       "\n",
       "                                     categories  \n",
       "0  'Other Sports', 'Dance', 'Sports & Outdoors'  \n",
       "1  'Other Sports', 'Dance', 'Sports & Outdoors'  \n",
       "2  'Other Sports', 'Dance', 'Sports & Outdoors'  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把item特征混合到训练集中\n",
    "df_all_merged = pd.merge(df_all,df_feat,on='item_id',how='left')\n",
    "df_all_merged[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1060f351",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_158152/3565603937.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  item_ids_list = torch.load(item_ids_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 加载item_ids_list: 48608个items\n",
      "   示例: ['0000031852', '0000032050', '0615302939']\n"
     ]
    }
   ],
   "source": [
    "item_ids_path = os.path.join(data_root, \"item_id.pt\")\n",
    "item_ids_list = torch.load(item_ids_path)\n",
    "print(f\"✅ 加载item_ids_list: {len(item_ids_list)}个items\")\n",
    "print(f\"   示例: {item_ids_list[:3]}\")\n",
    "item_to_position = {}\n",
    "for pos, item_id in enumerate(item_ids_list):\n",
    "    key = item_id.decode('utf-8') if isinstance(item_id, bytes) else str(item_id)\n",
    "    item_to_position[key] = pos\n",
    "df_all_merged['item_position'] = df_all_merged['item_id'].astype(str).map(\n",
    "    lambda x: item_to_position.get(x, 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ea3aff4-45b9-414d-9a27-8cc82a8a2472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dataset_type</th>\n",
       "      <th>sales_type</th>\n",
       "      <th>brand</th>\n",
       "      <th>categories</th>\n",
       "      <th>item_position</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>751626</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>24</td>\n",
       "      <td>1501</td>\n",
       "      <td>640</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>158994</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>24</td>\n",
       "      <td>1501</td>\n",
       "      <td>640</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>850430</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>24</td>\n",
       "      <td>1501</td>\n",
       "      <td>640</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id dataset_type  sales_type  brand  categories  \\\n",
       "0   751626        0        train          24   1501         640   \n",
       "1   158994        0        train          24   1501         640   \n",
       "2   850430        0        train          24   1501         640   \n",
       "\n",
       "   item_position  label  \n",
       "0              0      1  \n",
       "1              0      1  \n",
       "2              0      1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用labelencoder对稀疏特征进行编码\n",
    "encodered_columns = ['user_id', 'item_id', 'sales_type', 'brand','categories']\n",
    "lbe = LabelEncoder()\n",
    "for column in encodered_columns:\n",
    "    df_all_merged[column] = lbe.fit_transform(df_all_merged[column])\n",
    "columns = [col for col in df_all_merged.columns if col != 'label'] + ['label']\n",
    "df_all_merged = df_all_merged[columns]\n",
    "df_all_merged[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1da156f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48607"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_merged['item_id'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0730a5ad-a7ff-434b-8130-e60d743527b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.csv saved\n",
      "valid.csv saved\n",
      "test.csv saved\n"
     ]
    }
   ],
   "source": [
    "# 保存为csv和h5\n",
    "for dataset_type in dataset_types: \n",
    "    df_split = df_all_merged[df_all_merged['dataset_type'] == dataset_type].drop(columns=['dataset_type'])\n",
    "    output_path = f\"{output_root}/base_dataset\"\n",
    "    os.makedirs(output_path,exist_ok = True)\n",
    "    df_split.to_csv(os.path.join(output_path,f'{dataset_type}.csv'),index=False)\n",
    "    print(f\"{dataset_type}.csv saved\")\n",
    "    # with h5py.File(os.path.join(output_path,f'{dataset_type}.h5'), 'w') as f:\n",
    "    #     f.create_dataset('data', data=df_split.values,dtype='float64')\n",
    "    # print(f\"{dataset_type}.h5 saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667fb755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c0b9068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 加载item_ids_list: 56657个items\n",
      "   示例: ['0375829695', '0439855896', '0439893577']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_936137/3315848356.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  item_ids_list = torch.load(item_ids_path)\n"
     ]
    }
   ],
   "source": [
    "item_ids_path = os.path.join(data_root, \"item_id.pt\")\n",
    "item_ids_list = torch.load(item_ids_path)\n",
    "print(f\"✅ 加载item_ids_list: {len(item_ids_list)}个items\")\n",
    "print(f\"   示例: {item_ids_list[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d0e4f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3091269/3708196371.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  index = torch.load(\"/data2/wangzhongren/taolin_project/dataset/toys-split/moc_cbsize256_cbdim32_scala7_epoch500_index.pt\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 74, 132, 206,  ..., 223, 252,  89],\n",
       "        [132,  87,  86,  ..., 207,  86,  87],\n",
       "        [147,   6,  62,  ...,  82, 167, 213],\n",
       "        ...,\n",
       "        [212, 159,  44,  ..., 146, 147, 179],\n",
       "        [162, 153,  50,  ...,  18, 217, 145],\n",
       "        [  3,  87,  60,  ..., 187,  86,  87]], device='cuda:7')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = torch.load(\"/data2/wangzhongren/taolin_project/dataset/toys-split/moc_cbsize256_cbdim32_scala7_epoch500_index.pt\")\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f39d67d9-ecf9-4533-acd7-75844709e0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1230342,)\n",
      "(1230342,)\n",
      "(1230342,)\n",
      "(1230342,)\n",
      "(1230342,)\n",
      "(1230342,)\n",
      "(1230342,)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(\"/data2/wangzhongren/taolin_project/FuxiCTR/model_zoo/data/sports/train.h5\", 'r') as f:\n",
    "    for key in f.keys():\n",
    "        print(f[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f6e1a03-3e7a-496e-aea0-ebe5dd97060d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11272/3763878460.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  index = torch.load(\"/data2/wangzhongren/taolin_project/dataset/sports-split/moc_cbsize256_cbdim32_scala7_epoch500_index.pt\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([48608, 7])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = torch.load(\"/data2/wangzhongren/taolin_project/dataset/sports-split/moc_cbsize256_cbdim32_scala7_epoch500_index.pt\")\n",
    "index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7524c4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11272/1434103258.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  index = torch.load(\"/data2/wangzhongren/taolin_project/dataset/beauty-split/moc_cbsize256_cbdim32_scala7_epoch500_index.pt\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([47171, 7])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = torch.load(\"/data2/wangzhongren/taolin_project/dataset/beauty-split/moc_cbsize256_cbdim32_scala7_epoch500_index.pt\")\n",
    "index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5069b024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brand\n",
      "categories\n",
      "item_id\n",
      "item_position\n",
      "label\n",
      "sales_type\n",
      "user_id\n",
      "[1 1 1 1 1 1 2 2 2 2]\n",
      "[0. 0. 0. 0. 0. 0. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "path = '/data2/wangzhongren/taolin_project/FuxiCTR/model_zoo/data/sports/train.h5'\n",
    "with h5py.File(path, 'r') as f:\n",
    "    for key in f.keys():\n",
    "        print(key)\n",
    "    item_id = f['item_id'][:]\n",
    "    item_position = f['item_position'][:]\n",
    "    print(item_id[:10])\n",
    "    print(item_position[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d339b6cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "5    0\n",
       "6    1\n",
       "7    1\n",
       "8    1\n",
       "9    1\n",
       "Name: item_position, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/data2/wangzhongren/taolin_project/data/sports-split/base_dataset/train.csv'\n",
    "df = pd.read_csv(path)\n",
    "df['item_id'][:10]\n",
    "df['item_position'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f33214",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tencent_taolin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
