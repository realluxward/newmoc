{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "152f223f-b9f5-48cc-9c17-8bea2262ce08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4295293-b366-425f-b271-756260d3454e",
   "metadata": {},
   "source": [
    "#### 最原始的数据集（以toys为例）\n",
    "1. toys-split.item 是物品的信息数据集，每一个物品对应几个特征，id，title，price，categories，brand，sales_type，\n",
    "2. toys-split.train/valid/test.iter 是训练用数据集，每一条样本有一个uid，iid，rating，timestamp\n",
    "3. amazon-toys-games-filter_hdf5/norm_item_feat.h5 是一个llm的编码库，每一个item_id对应一个4096维的llm表征向量\n",
    "# 涛林代码中，删掉了price和timestamp特征，直接沿用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c89ca19-5f48-4ebf-af7c-478e7f124b43",
   "metadata": {},
   "source": [
    "### 1.对于item数据集中的每一个item，根据llm表，获得对应的llm表征编码，并保存为一个pt文件，给vae模型使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e15877e-2e7c-4b04-8d4f-14a13c25e6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/56657 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56657/56657 [00:08<00:00, 6666.52it/s]\n",
      "100%|██████████| 47171/47171 [00:08<00:00, 5337.50it/s]\n",
      "100%|██████████| 48608/48608 [00:07<00:00, 6898.28it/s]\n"
     ]
    }
   ],
   "source": [
    "datasets = [\"amazon-toys-games-filter\",\"amazon-beauty-filter\",\"amazon-sports-outdoors-filter\"]\n",
    "split_datasets = [\"toys-split\",\"beauty-split\",\"sports-split\"]\n",
    "\n",
    "for dataset,split_dataset in zip(datasets,split_datasets):\n",
    "    item_feat_path = f\"./{dataset}_hdf5/norm_item_feat.h5\"\n",
    "    item_feat_dict = h5py.File(item_feat_path,'r')\n",
    "    item_feat = []\n",
    "    with open(f\"/data2/wangzhongren/taolin_project/dataset/{split_dataset}/{split_dataset}.item\", 'r') as read_f:\n",
    "        lines = read_f.read().splitlines()\n",
    "        for line in tqdm(lines[1:]):\n",
    "            item_id = line.split(\"\\t\")[0]\n",
    "            if item_id not in item_feat_dict.keys():\n",
    "                print(\"this id has no llm emb\")\n",
    "                continue\n",
    "            item_feat.append(item_feat_dict[item_id][:])\n",
    "    item_feat = np.stack(item_feat)\n",
    "    item_feat = torch.from_numpy(item_feat)\n",
    "    # os.mkdirs(f\"./{split_dataset}\",exist_ok=True)\n",
    "    torch.save(item_feat, f\"./dataset/{split_dataset}/item_feat_input.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370a7365-3432-4ee8-9fd6-23f4e57c7f96",
   "metadata": {},
   "source": [
    "### 2.运行train_vae.py文件，给每一个id都生成对应的几个index，保存在moc_cbsize256_scala7_epoch100_index.pt这样的文件中"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c090346-5500-451b-b08b-7d40ba6d30e3",
   "metadata": {},
   "source": [
    "### 3.实际的数据生产"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "318064e7-8104-40f8-a877-5e2f7dc2e78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"./toys-split\"\n",
    "sample_filename = \"toys-split.item\"\n",
    "index_filename = \"moc_cbsize256_scala3_epoch100_index.pt\"\n",
    "cbsize = int(index_filename.split(\"_\")[1][6:])\n",
    "dataset_types = ['train','valid','test']\n",
    "dataname = 'toys-split'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "408b9b07-7430-4d08-837a-d2a40c0a4140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>sales_type</th>\n",
       "      <th>brand</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0375829695</td>\n",
       "      <td>Home &amp; Kitchen</td>\n",
       "      <td>Dr. Seuss</td>\n",
       "      <td>'Toys &amp; Games', 'Puzzles', 'Jigsaw Puzzles'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0439855896</td>\n",
       "      <td>Toys &amp; Games</td>\n",
       "      <td>Rock Ridge</td>\n",
       "      <td>'Toys &amp; Games', 'Novelty &amp; Gag Toys', 'Magic K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0439893577</td>\n",
       "      <td>missing</td>\n",
       "      <td>Scholastic</td>\n",
       "      <td>'Toys &amp; Games', 'Pretend Play', 'Dress Up &amp; Pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      item_id      sales_type       brand  \\\n",
       "0  0375829695  Home & Kitchen   Dr. Seuss   \n",
       "1  0439855896    Toys & Games  Rock Ridge   \n",
       "2  0439893577         missing  Scholastic   \n",
       "\n",
       "                                          categories  \n",
       "0        'Toys & Games', 'Puzzles', 'Jigsaw Puzzles'  \n",
       "1  'Toys & Games', 'Novelty & Gag Toys', 'Magic K...  \n",
       "2  'Toys & Games', 'Pretend Play', 'Dress Up & Pr...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对item数据集的特征列进行处理，包括填充缺失值，对类别的分割\n",
    "feat_keys = ['item_id', 'sales_type', 'brand','categories']\n",
    "sample_path = os.path.join(data_root,sample_filename)\n",
    "df_feat = pd.read_csv(sample_path, sep='\\t', header=0)\n",
    "df_feat.columns = [col.split(\":\")[0] for col in df_feat.keys()]\n",
    "df_feat= df_feat[feat_keys]\n",
    "# for i in range(3):\n",
    "#     df_feat[f'category_{i+1}'] = df_feat['categories'].str.split(',').str.get(i).str.strip().fillna('Unknown')\n",
    "# df_feat = df_feat.drop(columns=['categories'])\n",
    "df_feat['sales_type'] = df_feat['sales_type'].fillna(\"missing\")\n",
    "df_feat[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1041c25-12de-412c-b49d-4e0ee1effcd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3893847/2337616677.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  index = torch.load(index_path) # 获取index 是一个tensor向量，torch.Size([56657, 1])，如果有多个expert或者多层rq的话，就是torch.Size([56657, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([56657, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取index数据集，跟样本一样多的行，顺序是对应的\n",
    "index_path = os.path.join(data_root,index_filename)\n",
    "index = torch.load(index_path) # 获取index 是一个tensor向量，torch.Size([56657, 1])，如果有多个expert或者多层rq的话，就是torch.Size([56657, 3])\n",
    "index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cce9fbbc-2912-4136-986a-db805127c80f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>sales_type</th>\n",
       "      <th>brand</th>\n",
       "      <th>categories</th>\n",
       "      <th>moc_index_1</th>\n",
       "      <th>moc_index_2</th>\n",
       "      <th>moc_index_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0375829695</td>\n",
       "      <td>Home &amp; Kitchen</td>\n",
       "      <td>Dr. Seuss</td>\n",
       "      <td>'Toys &amp; Games', 'Puzzles', 'Jigsaw Puzzles'</td>\n",
       "      <td>36</td>\n",
       "      <td>147</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0439855896</td>\n",
       "      <td>Toys &amp; Games</td>\n",
       "      <td>Rock Ridge</td>\n",
       "      <td>'Toys &amp; Games', 'Novelty &amp; Gag Toys', 'Magic K...</td>\n",
       "      <td>231</td>\n",
       "      <td>102</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0439893577</td>\n",
       "      <td>missing</td>\n",
       "      <td>Scholastic</td>\n",
       "      <td>'Toys &amp; Games', 'Pretend Play', 'Dress Up &amp; Pr...</td>\n",
       "      <td>171</td>\n",
       "      <td>85</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      item_id      sales_type       brand  \\\n",
       "0  0375829695  Home & Kitchen   Dr. Seuss   \n",
       "1  0439855896    Toys & Games  Rock Ridge   \n",
       "2  0439893577         missing  Scholastic   \n",
       "\n",
       "                                          categories  moc_index_1  \\\n",
       "0        'Toys & Games', 'Puzzles', 'Jigsaw Puzzles'           36   \n",
       "1  'Toys & Games', 'Novelty & Gag Toys', 'Magic K...          231   \n",
       "2  'Toys & Games', 'Pretend Play', 'Dress Up & Pr...          171   \n",
       "\n",
       "   moc_index_2  moc_index_3  \n",
       "0          147            5  \n",
       "1          102          214  \n",
       "2           85          212  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把index特征融入到item表中\n",
    "expert_num = int(index_path.split(\"scala\")[1][:1])\n",
    "method = index_filename.split(\"_\")[0]\n",
    "for i in range(expert_num):\n",
    "    df_feat[f\"{method}_index_{i+1}\"] = index.cpu()[:,i]\n",
    "df_feat[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1a985bb-4925-44cb-af0d-7d40392b2f5e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3893847/2375479590.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{data_root}/{data_filename}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdf_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdf_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\":\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdf_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rating'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mdf_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'rating'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mdf_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dataset_type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mdf_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdf_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tencent_taolin/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5985\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5986\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5987\u001b[0m         ):\n\u001b[1;32m   5988\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5989\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "# 对训练用数据集进行初步处理，label,列名等等，检查是否有空值\n",
    "df_all = pd.DataFrame()\n",
    "for dataset_type in dataset_types:\n",
    "    data_filename = f\"{dataname}.{dataset_type}.inter\"\n",
    "    data_path = f\"{data_root}/{data_filename}\"\n",
    "    df_data = pd.read_csv(data_path, sep='\\t', header=0)\n",
    "    df_data.columns = [col.split(\":\")[0] for col in df_data.keys()]\n",
    "    df_data['label'] = (df_data['rating']>3).astype(int)\n",
    "    df_data = df_data.drop(columns = ['rating'])\n",
    "    df_data['dataset_type'] = dataset_type\n",
    "    df_all=df_all.append(df_data)\n",
    "df_all[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe7c89e-b60f-4d01-a2e7-085e664e1899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "      <th>dataset_type</th>\n",
       "      <th>sales_type</th>\n",
       "      <th>brand</th>\n",
       "      <th>category_1</th>\n",
       "      <th>category_2</th>\n",
       "      <th>category_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AYVR1MQCTNU5D</td>\n",
       "      <td>0375829695</td>\n",
       "      <td>1291939200</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>Home &amp; Kitchen</td>\n",
       "      <td>Dr. Seuss</td>\n",
       "      <td>'Toys &amp; Games'</td>\n",
       "      <td>'Puzzles'</td>\n",
       "      <td>'Jigsaw Puzzles'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A3CJHKFHHQJP2K</td>\n",
       "      <td>0375829695</td>\n",
       "      <td>1297209600</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>Home &amp; Kitchen</td>\n",
       "      <td>Dr. Seuss</td>\n",
       "      <td>'Toys &amp; Games'</td>\n",
       "      <td>'Puzzles'</td>\n",
       "      <td>'Jigsaw Puzzles'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A3638FINP26E8N</td>\n",
       "      <td>0375829695</td>\n",
       "      <td>1282521600</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>Home &amp; Kitchen</td>\n",
       "      <td>Dr. Seuss</td>\n",
       "      <td>'Toys &amp; Games'</td>\n",
       "      <td>'Puzzles'</td>\n",
       "      <td>'Jigsaw Puzzles'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          user_id     item_id   timestamp  label dataset_type      sales_type  \\\n",
       "0   AYVR1MQCTNU5D  0375829695  1291939200      1        train  Home & Kitchen   \n",
       "1  A3CJHKFHHQJP2K  0375829695  1297209600      0        train  Home & Kitchen   \n",
       "2  A3638FINP26E8N  0375829695  1282521600      0        train  Home & Kitchen   \n",
       "\n",
       "       brand      category_1 category_2        category_3  \n",
       "0  Dr. Seuss  'Toys & Games'  'Puzzles'  'Jigsaw Puzzles'  \n",
       "1  Dr. Seuss  'Toys & Games'  'Puzzles'  'Jigsaw Puzzles'  \n",
       "2  Dr. Seuss  'Toys & Games'  'Puzzles'  'Jigsaw Puzzles'  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把item特征混合到训练集中\n",
    "df_all_merged = pd.merge(df_all,df_feat,on='item_id',how='left')\n",
    "df_all_merged[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "880d0cdf-f040-43a0-98a8-31c9c187ce52",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_all_merged' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m lbe \u001b[38;5;241m=\u001b[39m LabelEncoder()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m encodered_columns:\n\u001b[0;32m----> 5\u001b[0m     df_all_merged[column] \u001b[38;5;241m=\u001b[39m lbe\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mdf_all_merged\u001b[49m[column])\n\u001b[1;32m      6\u001b[0m columns \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df_all_merged\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      7\u001b[0m df_all_merged \u001b[38;5;241m=\u001b[39m df_all_merged[columns]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_all_merged' is not defined"
     ]
    }
   ],
   "source": [
    "# 使用labelencoder对稀疏特征进行编码\n",
    "encodered_columns = ['user_id', 'item_id', 'timestamp', 'sales_type', 'brand','category_1', 'category_2', 'category_3']\n",
    "lbe = LabelEncoder()\n",
    "for column in encodered_columns:\n",
    "    df_all_merged[column] = lbe.fit_transform(df_all_merged[column])\n",
    "columns = [col for col in df_all_merged.columns if col != 'label'] + ['label']\n",
    "df_all_merged = df_all_merged[columns]\n",
    "df_all_merged[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "657eecf1-2779-424d-9224-ee2857fe03f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_all_merged' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 保存为csv和h5\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset_type \u001b[38;5;129;01min\u001b[39;00m dataset_types: \n\u001b[0;32m----> 3\u001b[0m     df_split \u001b[38;5;241m=\u001b[39m \u001b[43mdf_all_merged\u001b[49m[df_all_merged[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m dataset_type]\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_type\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m     output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_root\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex_filename[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(output_path,exist_ok \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_all_merged' is not defined"
     ]
    }
   ],
   "source": [
    "# 保存为csv和h5\n",
    "for dataset_type in dataset_types: \n",
    "    df_split = df_all_merged[df_all_merged['dataset_type'] == dataset_type].drop(columns=['dataset_type'])\n",
    "    output_path = f\"{data_root}/{index_filename[:-9]}\"\n",
    "    os.makedirs(output_path,exist_ok = True)\n",
    "    df_split.to_csv(os.path.join(output_path,f'{dataset_type}.csv'),index=False)\n",
    "    print(f\"{dataset_type}.csv saved\")\n",
    "    with h5py.File(os.path.join(output_path,f'{dataset_type}.h5'), 'w') as f:\n",
    "        f.create_dataset('data', data=df_split.values,dtype='float64')\n",
    "    print(f\"{dataset_type}.h5 saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefcd186-cb9b-4421-8f7a-badba8b7715a",
   "metadata": {},
   "source": [
    "### 4.feature_map构建\n",
    "（经检验，所有的列在train数据集能涵盖所有可能性，故只需要扫描train数据集即可）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c65fe15-4404-4a73-bb56-d368ee07538c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_map saved\n"
     ]
    }
   ],
   "source": [
    "# df_all_merged = df_all_merged.drop(columns=['dataset_type'])\n",
    "# 创建feature_map\n",
    "feature_specs = {}\n",
    "col_value_index = 0\n",
    "num_features = 0\n",
    "for index,col in enumerate(df_all_merged.columns):\n",
    "    if col in encodered_columns:\n",
    "        feature_specs[col] = {\n",
    "                    'source': '',\n",
    "                    'type':\"categorical\",\n",
    "                    'vocab_size': df_all_merged[col].nunique(),\n",
    "                    'index': index\n",
    "                }\n",
    "        num_features = num_features + df_all_merged[col].nunique()\n",
    "    elif col not in encodered_columns and col != 'label':\n",
    "        feature_specs[col] = {\n",
    "                    'source': '',\n",
    "                    'type':\"categorical\",\n",
    "                    'vocab_size': cbsize,\n",
    "                    'index': index\n",
    "                }\n",
    "        num_features = num_features + cbsize\n",
    "feature_map = {\n",
    "        'dataset_id': f'amazon_{dataname}_{index_filename[:-9]}',\n",
    "        'num_fields': len(df_all_merged.columns)-1,\n",
    "        # 'feature_len': len(feature_specs),\n",
    "        'num_features': num_features,\n",
    "        'feature_specs': feature_specs\n",
    "    }\n",
    "json.dump(feature_map, open(os.path.join(output_path,f'feature_map.json'), 'w'),indent=4)\n",
    "print(\"feature_map saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea82133-d12a-459d-81d9-8fd14f4ebe3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72b9c5dc-df83-4de6-b165-a19656e68fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id 772280 772280\n",
      "item_id 56650 56650\n",
      "timestamp 5020 5020\n",
      "sales_type 27 27\n",
      "brand 5996 5996\n",
      "categories 700 700\n",
      "moc_index_1 250 250\n",
      "moc_index_2 248 248\n",
      "moc_index_3 251 251\n",
      "label 2 2\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('./toys-split/moc_cbsize256_scala3_epoch100/train.csv')\n",
    "valid_data = pd.read_csv('./toys-split/moc_cbsize256_scala3_epoch100/valid.csv')\n",
    "test_data = pd.read_csv('./toys-split/moc_cbsize256_scala3_epoch100/test.csv')\n",
    "all_data = train_data.append(valid_data).append(test_data)\n",
    "for col in train_data.columns:\n",
    "    print(col,train_data[col].nunique(),all_data[col].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ffae280-5014-4403-bb86-9d2207aed027",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_all_merged' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-65ca7a36bd31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencodered_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_all_merged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'index'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mencodered_columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_all_merged' is not defined"
     ]
    }
   ],
   "source": [
    "encodered_columns = [col for col in df_all_merged.columns if 'index' not in col and col != 'label']\n",
    "encodered_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec024b81-64ad-4144-b09b-78f8edfbea13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1985ada-9aa5-492b-89f8-9e92c76099ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in the file: ['data']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.19573e+05, 0.00000e+00, 3.86100e+03, 2.30000e+01, 3.44300e+03,\n",
       "        3.54000e+02, 2.31000e+02, 1.02000e+02, 2.14000e+02, 0.00000e+00],\n",
       "       [5.26320e+04, 0.00000e+00, 3.92500e+03, 2.30000e+01, 3.44300e+03,\n",
       "        3.54000e+02, 2.31000e+02, 1.02000e+02, 2.14000e+02, 0.00000e+00],\n",
       "       [4.22370e+04, 0.00000e+00, 3.97700e+03, 2.30000e+01, 3.44300e+03,\n",
       "        3.54000e+02, 2.31000e+02, 1.02000e+02, 2.14000e+02, 0.00000e+00],\n",
       "       [7.68100e+04, 0.00000e+00, 2.68100e+03, 2.30000e+01, 3.44300e+03,\n",
       "        3.54000e+02, 2.31000e+02, 1.02000e+02, 2.14000e+02, 1.00000e+00],\n",
       "       [1.06183e+05, 1.00000e+00, 3.93400e+03, 2.60000e+01, 3.57900e+03,\n",
       "        4.34000e+02, 1.71000e+02, 8.50000e+01, 2.12000e+02, 0.00000e+00],\n",
       "       [6.79310e+04, 1.00000e+00, 3.79000e+03, 2.60000e+01, 3.57900e+03,\n",
       "        4.34000e+02, 1.71000e+02, 8.50000e+01, 2.12000e+02, 1.00000e+00],\n",
       "       [6.12690e+04, 1.00000e+00, 3.13800e+03, 2.60000e+01, 3.57900e+03,\n",
       "        4.34000e+02, 1.71000e+02, 8.50000e+01, 2.12000e+02, 1.00000e+00],\n",
       "       [1.18658e+05, 1.00000e+00, 3.88100e+03, 2.60000e+01, 3.57900e+03,\n",
       "        4.34000e+02, 1.71000e+02, 8.50000e+01, 2.12000e+02, 1.00000e+00],\n",
       "       [8.42270e+04, 1.00000e+00, 2.69300e+03, 2.60000e+01, 3.57900e+03,\n",
       "        4.34000e+02, 1.71000e+02, 8.50000e+01, 2.12000e+02, 1.00000e+00],\n",
       "       [1.27472e+05, 1.00000e+00, 3.42100e+03, 2.60000e+01, 3.57900e+03,\n",
       "        4.34000e+02, 1.71000e+02, 8.50000e+01, 2.12000e+02, 1.00000e+00],\n",
       "       [2.02760e+04, 1.00000e+00, 3.46400e+03, 2.60000e+01, 3.57900e+03,\n",
       "        4.34000e+02, 1.71000e+02, 8.50000e+01, 2.12000e+02, 1.00000e+00],\n",
       "       [9.50400e+03, 2.00000e+00, 3.12700e+03, 0.00000e+00, 4.54300e+03,\n",
       "        8.00000e+01, 3.30000e+01, 4.70000e+01, 1.25000e+02, 1.00000e+00],\n",
       "       [8.70340e+04, 2.00000e+00, 2.44100e+03, 0.00000e+00, 4.54300e+03,\n",
       "        8.00000e+01, 3.30000e+01, 4.70000e+01, 1.25000e+02, 1.00000e+00],\n",
       "       [5.84370e+04, 2.00000e+00, 3.91800e+03, 0.00000e+00, 4.54300e+03,\n",
       "        8.00000e+01, 3.30000e+01, 4.70000e+01, 1.25000e+02, 1.00000e+00],\n",
       "       [3.04640e+04, 2.00000e+00, 4.02000e+03, 0.00000e+00, 4.54300e+03,\n",
       "        8.00000e+01, 3.30000e+01, 4.70000e+01, 1.25000e+02, 1.00000e+00],\n",
       "       [6.97910e+04, 2.00000e+00, 2.87800e+03, 0.00000e+00, 4.54300e+03,\n",
       "        8.00000e+01, 3.30000e+01, 4.70000e+01, 1.25000e+02, 1.00000e+00],\n",
       "       [1.05397e+05, 2.00000e+00, 3.75900e+03, 0.00000e+00, 4.54300e+03,\n",
       "        8.00000e+01, 3.30000e+01, 4.70000e+01, 1.25000e+02, 1.00000e+00],\n",
       "       [1.15600e+05, 3.00000e+00, 3.52700e+03, 0.00000e+00, 1.14400e+03,\n",
       "        1.32000e+02, 3.30000e+01, 2.48000e+02, 1.25000e+02, 1.00000e+00],\n",
       "       [5.00430e+04, 3.00000e+00, 3.92000e+03, 0.00000e+00, 1.14400e+03,\n",
       "        1.32000e+02, 3.30000e+01, 2.48000e+02, 1.25000e+02, 1.00000e+00],\n",
       "       [1.22208e+05, 3.00000e+00, 3.72000e+03, 0.00000e+00, 1.14400e+03,\n",
       "        1.32000e+02, 3.30000e+01, 2.48000e+02, 1.25000e+02, 0.00000e+00]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"./toys-split/moc_cbsize256_scala3_epoch100/test.h5\"\n",
    "with h5py.File(file_path, \"r\") as h5_file:\n",
    "    # 列出所有顶层键\n",
    "    print(\"Keys in the file:\", list(h5_file.keys()))\n",
    "    \n",
    "    # 读取某个数据集\n",
    "    dataset_name = list(h5_file.keys())[0]  # 假设取第一个键\n",
    "    data = h5_file[dataset_name][:]\n",
    "data[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9f6f365-47cb-47d7-a52d-f15abb6c1ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1294950/3324600003.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  base = torch.load(\"/data2/wangzhongren/taolin_project/dataset/beauty-split/item_feat_input.pt\")\n",
      "/tmp/ipykernel_1294950/3324600003.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  t1 = torch.load(\"/data2/wangzhongren/taolin_project/dataset/beauty-split/me_cbsize256_cbdim32_scala1_epoch500_index.pt\")\n",
      "/tmp/ipykernel_1294950/3324600003.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  t2 = torch.load(\"/data2/wangzhongren/taolin_project/dataset/beauty-split/moc_cbsize256_cbdim32_scala1_epoch500_index.pt\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "base = torch.load(\"/data2/wangzhongren/taolin_project/dataset/beauty-split/item_feat_input.pt\")\n",
    "t1 = torch.load(\"/data2/wangzhongren/taolin_project/dataset/beauty-split/me_cbsize256_cbdim32_scala1_epoch500_index.pt\")\n",
    "t2 = torch.load(\"/data2/wangzhongren/taolin_project/dataset/beauty-split/moc_cbsize256_cbdim32_scala1_epoch500_index.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95653538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([47171, 4096]), torch.Size([47171, 1]), torch.Size([47171, 1]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base.shape,t1.shape,t2.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc165dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tencent_taolin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
